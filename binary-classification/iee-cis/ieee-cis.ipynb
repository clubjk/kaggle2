{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ca3c2-de20-427e-8093-09d1c79c9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one causes the Ubuntu python environment new_ml_env to die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b56884-428c-446a-9117-35ea80e411d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas numpy matplotlib seaborn scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a963a3f-01fb-4fab-bcdb-78a933f98632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       " 0        2987000        0          86400            68.5         W  13926   \n",
       " 1        2987001        0          86401            29.0         W   2755   \n",
       " 2        2987002        0          86469            59.0         W   4663   \n",
       " 3        2987003        0          86499            50.0         W  18132   \n",
       " 4        2987004        0          86506            50.0         H   4497   \n",
       " \n",
       "    card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
       " 0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       " 1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       " 2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       " 3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       " 4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
       " \n",
       "   V336  V337  V338  V339  \n",
       " 0  NaN   NaN   NaN   NaN  \n",
       " 1  NaN   NaN   NaN   NaN  \n",
       " 2  NaN   NaN   NaN   NaN  \n",
       " 3  NaN   NaN   NaN   NaN  \n",
       " 4  0.0   0.0   0.0   0.0  \n",
       " \n",
       " [5 rows x 394 columns],\n",
       "    TransactionID  id_01     id_02  id_03  id_04  id_05  id_06  id_07  id_08  \\\n",
       " 0        2987004    0.0   70787.0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       " 1        2987008   -5.0   98945.0    NaN    NaN    0.0   -5.0    NaN    NaN   \n",
       " 2        2987010   -5.0  191631.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       " 3        2987011   -5.0  221832.0    NaN    NaN    0.0   -6.0    NaN    NaN   \n",
       " 4        2987016    0.0    7460.0    0.0    0.0    1.0    0.0    NaN    NaN   \n",
       " \n",
       "    id_09  ...                id_31  id_32      id_33           id_34  id_35  \\\n",
       " 0    NaN  ...  samsung browser 6.2   32.0  2220x1080  match_status:2      T   \n",
       " 1    NaN  ...   mobile safari 11.0   32.0   1334x750  match_status:1      T   \n",
       " 2    0.0  ...          chrome 62.0    NaN        NaN             NaN      F   \n",
       " 3    NaN  ...          chrome 62.0    NaN        NaN             NaN      F   \n",
       " 4    0.0  ...          chrome 62.0   24.0   1280x800  match_status:2      T   \n",
       " \n",
       "   id_36 id_37  id_38  DeviceType                     DeviceInfo  \n",
       " 0     F     T      T      mobile  SAMSUNG SM-G892A Build/NRD90M  \n",
       " 1     F     F      T      mobile                     iOS Device  \n",
       " 2     F     T      T     desktop                        Windows  \n",
       " 3     F     T      T     desktop                            NaN  \n",
       " 4     F     T      T     desktop                          MacOS  \n",
       " \n",
       " [5 rows x 41 columns],\n",
       "    TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       " 0        3663549       18403224           31.95         W  10409  111.0   \n",
       " 1        3663550       18403263           49.00         W   4272  111.0   \n",
       " 2        3663551       18403310          171.00         W   4476  574.0   \n",
       " 3        3663552       18403310          284.95         W  10989  360.0   \n",
       " 4        3663553       18403317           67.95         W  18018  452.0   \n",
       " \n",
       "    card3       card4  card5  card6  ...  V330  V331  V332  V333 V334 V335  \\\n",
       " 0  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       " 1  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       " 2  150.0        visa  226.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       " 3  150.0        visa  166.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       " 4  150.0  mastercard  117.0  debit  ...   NaN   NaN   NaN   NaN  NaN  NaN   \n",
       " \n",
       "    V336  V337  V338  V339  \n",
       " 0   NaN   NaN   NaN   NaN  \n",
       " 1   NaN   NaN   NaN   NaN  \n",
       " 2   NaN   NaN   NaN   NaN  \n",
       " 3   NaN   NaN   NaN   NaN  \n",
       " 4   NaN   NaN   NaN   NaN  \n",
       " \n",
       " [5 rows x 393 columns],\n",
       "    TransactionID  id-01     id-02  id-03  id-04  id-05  id-06  id-07  id-08  \\\n",
       " 0        3663586  -45.0  280290.0    NaN    NaN    0.0    0.0    NaN    NaN   \n",
       " 1        3663588    0.0    3579.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       " 2        3663597   -5.0  185210.0    NaN    NaN    1.0    0.0    NaN    NaN   \n",
       " 3        3663601  -45.0  252944.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       " 4        3663602  -95.0  328680.0    NaN    NaN    7.0  -33.0    NaN    NaN   \n",
       " \n",
       "    id-09  ...                    id-31  id-32     id-33           id-34  \\\n",
       " 0    NaN  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       " 1    0.0  ...  chrome 67.0 for android   24.0  1280x720  match_status:2   \n",
       " 2    NaN  ...       ie 11.0 for tablet    NaN       NaN             NaN   \n",
       " 3    0.0  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       " 4    NaN  ...  chrome 67.0 for android    NaN       NaN             NaN   \n",
       " \n",
       "    id-35 id-36 id-37  id-38  DeviceType                   DeviceInfo  \n",
       " 0      F     F     T      F      mobile  MYA-L13 Build/HUAWEIMYA-L13  \n",
       " 1      T     F     T      T      mobile         LGLS676 Build/MXB48T  \n",
       " 2      F     T     T      F     desktop                  Trident/7.0  \n",
       " 3      F     F     T      F      mobile  MYA-L13 Build/HUAWEIMYA-L13  \n",
       " 4      F     F     T      F      mobile         SM-G9650 Build/R16NW  \n",
       " \n",
       " [5 rows x 41 columns],\n",
       "    TransactionID  isFraud\n",
       " 0        3663549      0.5\n",
       " 1        3663550      0.5\n",
       " 2        3663551      0.5\n",
       " 3        3663552      0.5\n",
       " 4        3663553      0.5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file paths\n",
    "base_dir = '/home/john/ai/kaggle/iee-cis/'\n",
    "train_transaction_file = os.path.join(base_dir, 'train_transaction.csv')\n",
    "train_identity_file = os.path.join(base_dir, 'train_identity.csv')\n",
    "test_transaction_file = os.path.join(base_dir, 'test_transaction.csv')\n",
    "test_identity_file = os.path.join(base_dir, 'test_identity.csv')\n",
    "sample_submission_file = os.path.join(base_dir, 'sample_submission.csv')\n",
    "\n",
    "# Load the datasets\n",
    "train_transaction = pd.read_csv(train_transaction_file)\n",
    "train_identity = pd.read_csv(train_identity_file)\n",
    "test_transaction = pd.read_csv(test_transaction_file)\n",
    "test_identity = pd.read_csv(test_identity_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)\n",
    "\n",
    "# Display the first few rows of each dataset to understand their structure\n",
    "train_transaction.head(), train_identity.head(), test_transaction.head(), test_identity.head(), sample_submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2d8eb-e9db-4069-8d91-bcdf5622a09b",
   "metadata": {},
   "source": [
    "Merge Transaction and Identity Data\n",
    "Since identity data is only available for a subset of transactions, you should merge the identity data with the transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a76a25-f0ce-4131-88c4-b05f8cc4b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge identity data with transaction data\n",
    "train_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test_df = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd07338-5619-43f5-8fe5-7681cee7737c",
   "metadata": {},
   "source": [
    "Handle Missing Values\n",
    "Impute Missing Values: For numerical features, you can impute missing values with the median or mean. For categorical features, you can fill missing values with a placeholder (e.g., 'missing')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2e4fed-d8dc-41d8-a02c-8240c6e3d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numerical columns with the median\n",
    "for col in train_df.columns:\n",
    "    if col in test_df.columns:  # Ensure the column exists in both train and test datasets\n",
    "        if train_df[col].dtype != 'object':\n",
    "            train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "            test_df[col].fillna(test_df[col].median(), inplace=True)\n",
    "    else:  # Handle the case where the column is not in the test set (like 'isFraud')\n",
    "        if train_df[col].dtype != 'object':\n",
    "            train_df[col].fillna(train_df[col].median(), inplace=True)\n",
    "\n",
    "# Fill missing values for categorical columns with a placeholder\n",
    "for col in train_df.columns:\n",
    "    if col in test_df.columns:  # Ensure the column exists in both train and test datasets\n",
    "        if train_df[col].dtype == 'object':\n",
    "            train_df[col].fillna('missing', inplace=True)\n",
    "            test_df[col].fillna('missing', inplace=True)\n",
    "    else:  # Handle the case where the column is not in the test set (like 'isFraud')\n",
    "        if train_df[col].dtype == 'object':\n",
    "            train_df[col].fillna('missing', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159259e-6e0b-4624-95c1-8168463acf8e",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "Transaction Amount: Create log-transformed features or binning to reduce skewness.\n",
    "Card Features: Consider creating new features like card1_card2_interaction by combining card1 and card2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cd6a68-78b8-4731-a760-eaf4fcb62644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Add this import statement\n",
    "\n",
    "# Log-transform the TransactionAmt\n",
    "train_df['TransactionAmt_log'] = train_df['TransactionAmt'].apply(lambda x: np.log1p(x))\n",
    "test_df['TransactionAmt_log'] = test_df['TransactionAmt'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "# Create interaction features\n",
    "train_df['card1_card2_interaction'] = train_df['card1'].astype(str) + '_' + train_df['card2'].astype(str)\n",
    "test_df['card1_card2_interaction'] = test_df['card1'].astype(str) + '_' + test_df['card2'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298dd193-3967-406e-8b65-727e6d69c086",
   "metadata": {},
   "source": [
    "Encoding Categorical Variables\n",
    "Label Encoding: For high cardinality categorical features like card1, card2, etc., consider label encoding.\n",
    "One-Hot Encoding: For low cardinality features like card4, card6, you can use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4998fc1a-2db9-43bc-8c6d-11f7cfc71b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode categorical columns with high cardinality\n",
    "for col in ['card1', 'card2', 'card3', 'card5', 'card1_card2_interaction']:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "    \n",
    "    # Transform test set with a fallback for unseen labels\n",
    "    test_df[col] = test_df[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
    "    le.classes_ = np.append(le.classes_, '<unknown>')  # Add an 'unknown' class for unseen labels\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "# One-hot encode lower cardinality categorical columns\n",
    "train_df = pd.get_dummies(train_df, columns=['card4', 'card6'])\n",
    "test_df = pd.get_dummies(test_df, columns=['card4', 'card6'])\n",
    "\n",
    "# Align columns in test set to match train set\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75fb8db3-961d-4717-9d99-016706ddfb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is: 590540\n"
     ]
    }
   ],
   "source": [
    "num_rows = train_df.shape[0]\n",
    "print(f\"The number of rows is: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd77b5-b1a6-43a7-88ff-76373433fc71",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "Correlation Analysis: You can remove features with low or no correlation with the target variable.\n",
    "Variance Threshold: You may want to remove features with very low variance as they may not contribute much to the model. Select Only Numeric Columns for Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b9e259-491b-4c65-b112-58c1b38f5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure only numeric columns are used for correlation\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate correlations only for numeric columns\n",
    "correlation = train_df[numeric_cols].corr()['isFraud'].abs()\n",
    "\n",
    "# Set a threshold to identify columns with low correlation\n",
    "correlation_threshold = 0.01\n",
    "to_drop = correlation[correlation < correlation_threshold].index.tolist()\n",
    "\n",
    "# Drop those columns from both train and test sets\n",
    "train_df.drop(columns=to_drop, inplace=True)\n",
    "test_df.drop(columns=to_drop, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8b137-04bf-4128-8c90-9ada69bf51ad",
   "metadata": {},
   "source": [
    "Train-Test Split\n",
    "Prepare your features and target variable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ced634-33d6-40b5-9b84-f0b906cdff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_train = train_df.drop(['isFraud', 'TransactionID'], axis=1)\n",
    "y_train = train_df['isFraud']\n",
    "\n",
    "X_test = test_df.drop(['isFraud', 'TransactionID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb70a13-1988-4ca4-93a0-e40fde989e9b",
   "metadata": {},
   "source": [
    " Model Training\n",
    "Now you can proceed with training a model using your prepared data. Gradient Boosting models like LightGBM, XGBoost, or CatBoost are popular choices for this type of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c58758-d552-492e-baff-90c6a5edc677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/john/miniconda3/envs/new_ml_env/lib/python3.8/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/john/miniconda3/envs/new_ml_env/lib/python3.8/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/john/miniconda3/envs/new_ml_env/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26080dc3-ee2f-4fca-bbc6-44c30d0b5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860d31d-f862-45a8-8fd3-87bdc20abd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load your data\n",
    "train_transaction = pd.read_csv('/home/john/ai/kaggle/iee-cis/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/home/john/ai/kaggle/iee-cis/train_identity.csv')\n",
    "\n",
    "# Merge the identity data with transaction data\n",
    "train_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "# Define target and features\n",
    "y_train = train_df['isFraud']\n",
    "X_train = train_df.drop(columns=['isFraud', 'TransactionID'])\n",
    "\n",
    "# Fill missing values (example with a placeholder, or you can use other methods)\n",
    "X_train.fillna(-999, inplace=True)\n",
    "\n",
    "# Identify all categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply LabelEncoder to each categorical column\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train_split, X_valid, y_train_split, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, run the grid search for RandomForest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_params, cv=3, scoring='roc_auc', verbose=3, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "print(f\"Best score for RandomForest: {rf_grid_search.best_score_}\")\n",
    "print(f\"Best parameters for RandomForest: {rf_grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6318ba-24ff-4ca2-ac94-ef5529187612",
   "metadata": {},
   "source": [
    "Code to Generate Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf60617-0f44-43aa-9b89-8b8688380050",
   "metadata": {},
   "source": [
    "3 models grid search pick the best one and create a submissions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "490d9b78-265d-48e6-8c1a-110781df5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost --timeout 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10acd995-1dcd-4ac6-bbc0-21166fc7721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24c8aea-89c9-491c-88e4-b9a259c8e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f9531-41c1-4d45-a5dd-d77c6dc90c6b",
   "metadata": {},
   "source": [
    " Run Grid Search for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e08083-45ca-44df-9a3a-d122f937d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cc427a-ec48-49b8-ada3-2e135a7e05fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best score for RandomForest: 0.8668713936940357\n",
      "Best parameters for RandomForest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load your data\n",
    "train_transaction = pd.read_csv('/home/john/ai/kaggle/iee-cis/train_transaction.csv')\n",
    "train_identity = pd.read_csv('/home/john/ai/kaggle/iee-cis/train_identity.csv')\n",
    "\n",
    "# Merge the identity data with transaction data\n",
    "train_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "# Define target and features\n",
    "y_train = train_df['isFraud']\n",
    "X_train = train_df.drop(columns=['isFraud', 'TransactionID'])\n",
    "\n",
    "# Fill missing values (example with a placeholder, or you can use other methods)\n",
    "X_train.fillna(-999, inplace=True)\n",
    "\n",
    "# Identify all categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply LabelEncoder to each categorical column\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train_split, X_valid, y_train_split, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, run the grid search for RandomForest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_params, cv=3, scoring='roc_auc', verbose=3, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "print(f\"Best score for RandomForest: {rf_grid_search.best_score_}\")\n",
    "print(f\"Best parameters for RandomForest: {rf_grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3ae6e-e228-4429-b0a6-e6bcdbe2383e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_ml_env)",
   "language": "python",
   "name": "new_ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
